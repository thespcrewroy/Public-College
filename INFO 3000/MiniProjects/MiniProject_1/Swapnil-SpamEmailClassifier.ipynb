{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Spam Email Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset “spam_ham_dataset.csv” contains the text of different emails a company has received over a period. Some are spam and some are not and are labeled accordingly. There is a label column which says ‘ham’ / ‘spam’ and another column called ‘class’ which represents it as 1s and 0s. Use the class column as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Read the data using pandas read_CSV method'''\n",
    "df = pd.read_csv('spam_ham_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " label      0\n",
      "message    0\n",
      "class      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Check for missing values'''\n",
    "missing_values = df.isnull().sum() # Count the number of missing values in each column\n",
    "print(f\"Missing values:\\n {missing_values}\") # Print the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop rows which have missing values'''\n",
    "df_cleaned = df.dropna() # Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject : enron methanol ; meter # : 988291 th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject : hpl nom for january 9 2001 ( see att...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject : neon retreat ho ho ho we re around t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>subject : photoshop window office . cheap . ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>subject : re : indian spring this deal is to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  class\n",
       "0   ham  subject : enron methanol ; meter # : 988291 th...      0\n",
       "1   ham  subject : hpl nom for january 9 2001 ( see att...      0\n",
       "2   ham  subject : neon retreat ho ho ho we re around t...      0\n",
       "3  spam  subject : photoshop window office . cheap . ma...      1\n",
       "4   ham  subject : re : indian spring this deal is to b...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert all text to lowercase, remove punctuations with commas or apostrophes, and lemmanize the text using the WordNetLemmatizer'''\n",
    "def clean_text(text): # Define a function to preprocess the text\n",
    "    cleaned_text = \"\" # Initialize an empty string to store the filtered text\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuations or apostrophes\n",
    "    text = text.replace(\",\", \"\").replace(\"'\", \"\") # Remove commas and apostrophes\n",
    "\n",
    "    # Create the stop words set\n",
    "    lemmatizer = WordNetLemmatizer() # Initialize the WordNetLemmatizer\n",
    "    text = word_tokenize(text) # Tokenize the text into words\n",
    "    text = [lemmatizer.lemmatize(word) for word in text] # Lemmatize the words in the string\n",
    "\n",
    "    # Join the lemmanized words back into a single string\n",
    "    cleaned_text = \" \".join(text)\n",
    "\n",
    "    return cleaned_text # Return the cleaned text\n",
    "\n",
    "# Apply the preprocessing function to the text column\n",
    "df_cleaned[\"message\"] = df_cleaned[\"message\"].apply(clean_text)\n",
    "\n",
    "# Print the preprocessed data\n",
    "print(\"Preprocessed Data:\")\n",
    "df_cleaned.head() # Print the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "0    subject : enron methanol ; meter # : 988291 th...\n",
      "1    subject : hpl nom for january 9 2001 ( see att...\n",
      "2    subject : neon retreat ho ho ho we re around t...\n",
      "3    subject : photoshop window office . cheap . ma...\n",
      "4    subject : re : indian spring this deal is to b...\n",
      "Name: message, dtype: object\n",
      "Labels:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Create the X and y datasets consisting of the features and labels'''\n",
    "X = df_cleaned['message'] # Features\n",
    "y = df_cleaned['class'] # Labels\n",
    "print(f\"Features:\\n{X.head()}\\nLabels:\\n{y.head()}\") # Print the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split them into training and testing datasets'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42) # Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparseform\n",
      "0       subject : enron methanol ; meter # : 988291 th...\n",
      "1       subject : hpl nom for january 9 2001 ( see att...\n",
      "2       subject : neon retreat ho ho ho we re around t...\n",
      "3       subject : photoshop window office . cheap . ma...\n",
      "4       subject : re : indian spring this deal is to b...\n",
      "                              ...                        \n",
      "5166    subject : put the 10 on the ft the transport v...\n",
      "5167    subject : 3 / 4 / 2000 and following noms hpl ...\n",
      "5168    subject : calpine daily gas nomination > > jul...\n",
      "5169    subject : industrial worksheet for august 2000...\n",
      "5170    subject : important online banking alert dear ...\n",
      "Name: message, Length: 5171, dtype: object\n",
      "\n",
      "Features\n",
      "['00' '000' '0000' ... 'zzo' 'zzocb' 'zzsyt']\n",
      "\n",
      "Shape: (4395, 44088)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Instantiate a TfidfVectorizer object and fit it to the training data'''\n",
    "vectorizer = TfidfVectorizer(stop_words='english') # Initialize the TfidfVectorizer object\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # Fit the vectorizer to the training data\n",
    "print(f\"Sparseform\\n{X}\\n\\nFeatures\\n{vectorizer.get_feature_names_out()}\\n\\nShape: {X_train_tfidf.shape}\\n\") # print the sparse matrix, features, and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the testing data\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian Naive Bayes Classifier:\n",
      "Accuracy: 0.9523195876288659\n",
      "Confusion Matrix:\n",
      "[[535  18]\n",
      " [ 19 204]]\n",
      "\n",
      "Multinomial Naive Bayes Classifier:\n",
      "Accuracy: 0.9265463917525774\n",
      "Confusion Matrix:\n",
      "[[552   1]\n",
      " [ 56 167]]\n",
      "\n",
      "RandomForest Classifier:\n",
      "Accuracy: 0.9780927835051546\n",
      "Confusion Matrix:\n",
      "[[544   9]\n",
      " [  8 215]]\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classifier\n",
    "gnb = GaussianNB() # Initialize the Gaussian Naive Bayes Classifier\n",
    "X_train_tfidf_dense = X_train_tfidf.toarray() # Convert the sparse matrix to a dense matrix\n",
    "X_test_tfidf_dense = X_test_tfidf.toarray() # Convert the sparse matrix to a dense matrix\n",
    "gnb.fit(X_train_tfidf_dense, y_train) # Fit the Gaussian Naive Bayes Classifier to the training data\n",
    "y_pred_gnb = gnb.predict(X_test_tfidf_dense) # Predict the labels of the testing data\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb) # Calculate the accuracy of the model\n",
    "conf_matrix_gnb = confusion_matrix(y_test, y_pred_gnb) # Calculate the confusion matrix\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_gnb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_gnb)\n",
    "\n",
    "# Multinomial Naive Bayes Classifier\n",
    "mnb = MultinomialNB() # Initialize the Multinomial Naive Bayes Classifier\n",
    "mnb.fit(X_train_tfidf, y_train) # Fit the Multinomial Naive Bayes Classifier to the training data\n",
    "y_pred_mnb = mnb.predict(X_test_tfidf) # Predict the labels of the testing data\n",
    "accuracy_mnb = accuracy_score(y_test, y_pred_mnb) # Calculate the accuracy of the model\n",
    "conf_matrix_mnb = confusion_matrix(y_test, y_pred_mnb) # Calculate the confusion matrix\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes Classifier:\") # Print the results\n",
    "print(\"Accuracy:\", accuracy_mnb) # Print the accuracy\n",
    "print(\"Confusion Matrix:\") # Print the confusion matrix\n",
    "print(conf_matrix_mnb) # Print the confusion matrix\n",
    "\n",
    "# RandomForest Classifier\n",
    "rfc = RandomForestClassifier(random_state=42) # Initialize the RandomForest Classifier\n",
    "rfc.fit(X_train_tfidf, y_train) # Fit the RandomForest Classifier to the training data\n",
    "y_pred_rfc = rfc.predict(X_test_tfidf) # Predict the labels of the testing data\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred_rfc) # Calculate the accuracy of the model\n",
    "conf_matrix_rfc = confusion_matrix(y_test, y_pred_rfc) # Calculate the confusion matrix\n",
    "\n",
    "print(\"\\nRandomForest Classifier:\") # Print the results\n",
    "print(\"Accuracy:\", accuracy_rfc) # Print the accuracy\n",
    "print(\"Confusion Matrix:\") # Print the confusion matrix\n",
    "print(conf_matrix_rfc) # Print the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest is the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for new emails:\n",
      "                                             message  predicted_class\n",
      "0  hello george how about a game of tennis tomorr...                1\n",
      "1          hello click here if you want drug tonight                1\n",
      "2    we offer free viagra ! ! ! click here now ! ! !                1\n",
      "3           dear sara i prepared the annual report .                1\n",
      "4           hi david will we go for cinema tonight ?                1\n",
      "5                 best holiday offer only here ! ! !                1\n",
      "6                        sir waiting for your mail .                1\n",
      "7                       # @ photoshop a fake image !                1\n",
      "8                   no problem . how are you doing ?                1\n"
     ]
    }
   ],
   "source": [
    "'''Pick the best model and get predictions for new emails'''\n",
    "# New email data\n",
    "emails = [\n",
    "    \"Hello George, how about a game of tennis tomorrow?\",\n",
    "    \"Hello, click here if you want drugs tonight\",\n",
    "    \"We offer free viagra!!! Click here now!!!\",\n",
    "    \"Dear Sara, I prepared the annual report.\",\n",
    "    \"Hi David, will we go for cinema tonight?\",\n",
    "    \"Best holidays offers only here!!!\",\n",
    "    'Sir, Waiting for your mail.',\n",
    "    '#@Photoshop a fake image!',\n",
    "    'No problem. How are you doing?'\n",
    "]\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "email_df = pd.DataFrame(emails, columns=['message']) # Convert the list to a DataFrame\n",
    "\n",
    "# Apply the clean_text function to the new email data\n",
    "email_df['message'] = email_df['message'].apply(clean_text) # Apply the clean_text function to the new email data\n",
    "\n",
    "# Transform the new email data using the TF-IDF vectorizer\n",
    "emails_tfidf = vectorizer.transform(email_df['message']) # Transform the new email data using the TF-IDF vectorizer\n",
    "\n",
    "# Predict the classes of the new emails\n",
    "email_predictions = rfc.predict(emails_tfidf) # Predict the classes of the new emails\n",
    "email_df['predicted_class'] = email_predictions # Add the predicted classes to the DataFrame\n",
    "\n",
    "print(\"\\nPredictions for new emails:\") # Print the predictions for the new emails\n",
    "print(email_df) # Print the predictions for the new emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Which model performed the best and what were the metrics for that model?*** <br>\n",
    "* Ans) Random performed the bestnd its metrics got 97% accuracy, with roughly an equal amount of false positives adn false negatives.\n",
    "\n",
    "***Looking at the new data for prediction, you should have a sense for which is spam\n",
    "and which is not – how did your model do against conclusions?***\n",
    "* Ans) my model predicted all the emails as spam. Clearly, this is not right and perplexing as my random forest classifier had the highest accuracy score out of all the models.\n",
    "\n",
    "***Why did I ask you not remove other types of punctuation?***\n",
    "* Ans) Punctuation like exclamation points, question marks, @'s and other symbols are fundamental indicators of spam emails, hence, they should not be removed during preproccessing.\n",
    "\n",
    "***What were some of the challenges you faced in doing this MP and what were the\n",
    "key learnings?***\n",
    "* Ans) The big chellenge I faced was with the Tf-IDF surprisingly where there was a value error. I learned that the value error was due to me combining the label and message for the features, when tf-idf only works for one string of text and not a combined double feature array of two different strings. Another issue is the overfitting of my random forest classifier which is supposed to be my 'best' algorithm. Other than that I had no issues when implementing these algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
