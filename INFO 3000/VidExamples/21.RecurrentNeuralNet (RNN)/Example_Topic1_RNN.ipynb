{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de644b63-3fe6-442c-9877-e1c95a4bcfcd",
   "metadata": {},
   "source": [
    "### RNN Example ###\n",
    "\n",
    "Here is a simple example of an RNN implemented in PyTorch. In this example, we will build a character-level RNN model that is trained to predict the next character in a sequence, given a sequence of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26555cd2-1a47-4fec-9a10-956f034be0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import torch # import pytorch\n",
    "import torch.nn as nn # import neural network module\n",
    "import string # import string module (standard python library that has all ascii characters)\n",
    "import random # import random module (standard python library that has all random functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f5cbe2e-5a80-42a2-89ea-702bdec4194a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11c57c130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) # set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82519733-dff2-4e51-9fe3-79d29458ddf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. \n",
      "Total charaters = 54\n"
     ]
    }
   ],
   "source": [
    "# Create a list of alphabets\n",
    "\n",
    "#all_characters = string.printable  # This command generates all the characters\n",
    "all_characters = string.ascii_letters + '.' + ' ' # This command generates all ascii letters, period and space\n",
    "print(f\"Characters:\\n{all_characters}\") # print all characters\n",
    "\n",
    "n_characters = len(all_characters) # get the number of characters\n",
    "print(f\"Total charaters = {n_characters}\") # print the number of characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d001f3fd-bd29-41bb-a622-ecdf79d030f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: This is a larger dataset. It contains many more sentences and charac \n",
      "Test:  ters than before. \n",
      "Test_Length:  17\n"
     ]
    }
   ],
   "source": [
    "# Whole dataset\n",
    "dataset = \"This is a larger dataset. It contains many more sentences and characters than before.\"\n",
    "\n",
    "# Split dataset into 80% training and 20% validation\n",
    "train_size = int(len(dataset) * 0.8) # initialize a n\n",
    "train_data = dataset[:train_size] # get the first 80% of the dataset for training\n",
    "valid_data = dataset[train_size:] # get the last 20% of the dataset for validation\n",
    "\n",
    "print('Train:', train_data,'\\nTest: ',valid_data, '\\nTest_Length: ', len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e144a9c-591e-4366-87a5-2950c10cd0cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Parameters requred to instantiate the layer ####\n",
    "When you create an instance of the nn.RNN class in PyTorch, you need to provide a few parameters. These parameters help to define the structure and function of the recurrent neural network. Here are the main parameters:\n",
    "\n",
    "## Required\n",
    "    input_size: The number of expected features in the input x. The input_size is equal to the number of unique characters in our training data (n_characters), which we got from converting our text data into a set of unique characters. In our case it is 54.\n",
    "\n",
    "    hidden_size: The number of features in the hidden state h. This essentially represents the \"memory\" of the RNN and can be thought of as the number of \"neurons\" or \"nodes\" in the hidden layer of the RNN. You can set this to any number you like, but keep in mind that larger numbers will increase the complexity and computational requirements of the model.\n",
    "\n",
    "    num_layers: Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and producing the final results. Default: 1\n",
    "\n",
    "## Not-Required\n",
    "    nonlinearity: The non-linearity to use ('tanh' or 'relu'). Default: 'tanh'\n",
    "\n",
    "    bias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "\n",
    "    batch_first: If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "\n",
    "    dropout: If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "\n",
    "    bidirectional: If True, becomes a bidirectional RNN. Default: False\n",
    "\n",
    "For most simple use cases, you'll probably just need to set input_size, hidden_size, and num_layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f452ad-aefd-4aa9-8213-6f3bdbd0faf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the RNN model\n",
    "# Remember that RNN gets 2 inputs - actual data and predicted value from previous time step\n",
    "\n",
    "class RNN(nn.Module): # Create a class RNN that inherits from nn.Module\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1): # Define the constructor\n",
    "        super(RNN, self).__init__() # Call the constructor of the parent class to initialize the object\n",
    "        \n",
    "        # Define all the details of the RNN architecture\n",
    "        self.input_size = input_size # number of expected features (unique characters) in the training data\n",
    "        self.hidden_size = hidden_size # number of features (nodes/neurons) in the hidden state (hidden layer) where the larger the hidden size, the more complex patterns the model can capture\n",
    "        self.output_size = output_size # number of expected features (unique characters) in the output data\n",
    "        self.n_layers = n_layers # number of recurrent layers (stacked RNNs) where the larger the number of layers, the more complex patterns the model can capture\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) # Converts text information into vectorial representation to be fed into the RNN (python community has pre-trained embeddings like word2vec, glove, etc. however, we will train our own embeddings here)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers) # Define the RNN layer where the first argument is the number of features in the input data, the second argument is the number of features in the hidden state, and the third argument is the number of RNN layers\n",
    "        self.decoder = nn.Linear(hidden_size, output_size) # define the Linear layer model and x activation function to convert the output of the RNN layer to the output size\n",
    "\n",
    "    def forward(self, input, hidden): # (input = current input character, hidden = previous state) - forward pass of the RNN\n",
    "        embedded = self.embedding(input.view(1, -1)) # reshape so it is 1 row, and then send it through the embedding layer to generate embeddings and convert it into a vector\n",
    "        output, hidden = self.rnn(embedded, hidden) # feed the embedded input and hidden state into the RNN layer to get the output and hidden state\n",
    "        output = self.decoder(output.view(1, -1)) # send the output to the linear layer to predict the next character in the sequence\n",
    "        return output, hidden # return the output and the hidden layer for the next iteration\n",
    "    \n",
    "    # Initialize the hidden layer parameters\n",
    "    def init_hidden(self): # initialize some values for the hidden state for the first time step\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size) # initialize the hidden state with zeros\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482d0c8-917e-4545-af20-f2f3efe3c436",
   "metadata": {
    "tags": []
   },
   "source": [
    "To train this RNN we need to create input and target tensors for each sequence in our dataset. Here we use a sequence length of n characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce20158-b299-4305-a796-ff9cdf8e5cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create as a numeric representation, of first to last characters (not including EOS) for input\n",
    "def char_tensor(string): # convert string to tensor\n",
    "    tensor = torch.zeros(len(string)).long() # initialize a tensor with zeros based on the length of the string\n",
    "    for c in range(len(string)): # loop through the string\n",
    "        tensor[c] = all_characters.index(string[c]) # get the index of the character in the string and assign it to the tensor\n",
    "    return tensor # return the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97d37f5-4997-4442-9c6d-035e36c1cfda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26, 11, 11, 53,  8, 18, 53, 22,  4, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "# Encode the string into a tensor\n",
    "print(char_tensor('All is well')) # takes each alphabet and spaces, and assigns the index to teh corresponding ascii character's index from the all_characters list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a9b6d1-8c9f-4d8d-9ad4-8c1a12977cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create randomly sequenced train and test data\n",
    "# These functions take in the parameter chunk length (should be less than or equal to the training or test data length)\n",
    "\n",
    "# Train data generation function\n",
    "def random_training_set(): # make feature and label pairs for training data\n",
    "    if chunk_len < len(train_data): # check if the chunk length is less than the length of the training data (chunk - get a random index from the training data as a starting point, and then get a chunk of data from that starting point)\n",
    "        start_index = random.randint(0, len(train_data) - chunk_len - 1) # pick a the random start index between the start and of the training data and the end of it minus the chunk length minus 1\n",
    "        end_index = start_index + chunk_len # get the end index by adding the chunk length to the start index\n",
    "    else: # error if chunk is greater than the training data length, so...\n",
    "        start_index = 0 # set the start index to 0\n",
    "        end_index = len(train_data) # set the end index to the length of the training data\n",
    "    \n",
    "    chunk = train_data[start_index:end_index] # get the chunk of data from the training data\n",
    "    inp = char_tensor(chunk[:-1]) # get the input tensor by taking all the characters in the chunk except the last one\n",
    "    target = char_tensor(chunk[1:]) # get the target tensor by taking all the characters in the chunk except the first one\n",
    "    return inp, target # return the input and target tensors (input - training features - the vectorized representation of the word, target - labels - the ascii characters)\n",
    "\n",
    "\n",
    "# Test data generation function\n",
    "def random_valid_set(): # make feature and label pairs for test data\n",
    "    if chunk_len < len(valid_data): # check if the chunk length is less than the length of the test data (chunk - get a random index from the test data as a starting point, and then get a chunk of data from that starting point)\n",
    "        start_index = random.randint(0, len(valid_data) - chunk_len - 1) # pick a the random start index between the start and of the test data and the end of it minus the chunk length minus 1\n",
    "        end_index = start_index + chunk_len # get the end index by adding the chunk length to the start index\n",
    "    else: # error if chunk is greater than the test data length, so...\n",
    "        start_index = 0 # set the start index to 0\n",
    "        end_index = len(valid_data) # set the end index to the length of the test data\n",
    "    chunk = valid_data[start_index:end_index] # get the chunk of data from the test data\n",
    "    inp = char_tensor(chunk[:-1]) # get the input tensor by taking all the characters in the chunk except the last one\n",
    "    target = char_tensor(chunk[1:]) # get the target tensor by taking all the characters in the chunk except the first one\n",
    "    return inp, target # return the input and target tensors (input - test features - the vectorized representation of the word, target - labels - the ascii characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48552b2-ef2e-40c7-a0f3-3a4147740840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let us implement our training function\n",
    "# Takes input and target values, runs it through the RNN modeland returns the average loss per chunk length of data\n",
    "\n",
    "def train(inp, target): # training function - take the input and target tensors\n",
    "    hidden = rnn.init_hidden() # initialize the hidden input (first time step / first input)\n",
    "    rnn.zero_grad() # zero the gradients of the RNN model\n",
    "    loss = 0 # initialize the loss to 0\n",
    "\n",
    "    for c in range(len(inp)): # loop through the input tensor (sentence dataset)\n",
    "        output, hidden = rnn(inp[c], hidden) # feed the input character by character and the hidden state into the RNN model to get the output and hidden state\n",
    "        loss += loss_fn(output, target[c].unsqueeze(0)) # use the loss function model to calculate the loss between the output and the target\n",
    "\n",
    "    loss.backward() # backpropagate the loss\n",
    "    optimizer.step() # optimize the gradients\n",
    "\n",
    "    return loss.data.item() / chunk_len # return the loss per chunk length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83012ef5-8471-4604-8f8e-b8de95764220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation data function\n",
    "# Here we simple feed the validation data to the RNN model and copmpare the prediction to the actual value\n",
    "# Returns lost per validation data chunk length\n",
    "\n",
    "def evaluate(inp, valid_target): # test function - take the input and target tensors\n",
    "    hidden = rnn.init_hidden() # initialize the hidden input (first time step / first input)\n",
    "    loss = 0 # initialize the loss to 0\n",
    "    for c in range(len(inp)): # loop through the input tensor (sentence dataset)\n",
    "        output, hidden = rnn(inp[c], hidden) # feed the input character by character and the hidden state into the RNN model to get the output and hidden state\n",
    "        loss += loss_fn(output, valid_target[c].unsqueeze(0)) # use the loss function model to calculate the loss between the output and the target\n",
    "    return loss.data.item() / chunk_len # return the loss per chunk length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905c391-6dd1-4ad7-9942-4bef9222eb60",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Calling functions within function which return tuple values ####\n",
    "\n",
    "The asterisk (*) in Python has a few different meanings depending on the context, but in this case, when it's used in a function call like train(*random_training_set()), it's used for unpacking the elements of random_training_set().\n",
    "\n",
    "In Python, the single asterisk (*) operator can be used in a function call to unpack an iterable into positional arguments passed to the function.\n",
    "\n",
    "In our case, random_training_set() returns a tuple of two elements: inp and target. When we call train(*random_training_set()), the * operator unpacks this tuple, and passes the elements of the tuple as separate arguments to the train function.\n",
    "\n",
    "This is the same as:\n",
    "\n",
    "inp, target = random_training_set()<br>train(inp, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3af55f6-5e1d-4017-b97e-80d38515231e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, Training Loss: 1.5496, Validation Loss: 1.0214\n",
      "Epoch: 2000, Training Loss: 0.8298, Validation Loss: 1.0342\n",
      "Epoch: 3000, Training Loss: 0.3885, Validation Loss: 1.0668\n",
      "Epoch: 4000, Training Loss: 0.3468, Validation Loss: 1.1076\n",
      "Epoch: 5000, Training Loss: 0.1540, Validation Loss: 1.1524\n",
      "Epoch: 6000, Training Loss: 0.0623, Validation Loss: 1.1929\n",
      "Epoch: 7000, Training Loss: 0.0558, Validation Loss: 1.2375\n",
      "Epoch: 8000, Training Loss: 0.0366, Validation Loss: 1.2898\n",
      "Epoch: 9000, Training Loss: 0.0252, Validation Loss: 1.3426\n",
      "Epoch: 10000, Training Loss: 0.0110, Validation Loss: 1.3854\n"
     ]
    }
   ],
   "source": [
    "# Create the training loop by calling the functions\n",
    "\n",
    "# set variable values\n",
    "n_epochs = 10000 # overfit the very small dataset, so we can see the model learning\n",
    "print_every = 1000 # print the loss every 1000 epochs\n",
    "hidden_size = 200 # number of neurons in the hidden layer\n",
    "n_layers = 1 # number of RNN layers\n",
    "lr = 0.00001 # very low learning rate because we have so many epochs and are overfitting\n",
    "chunk_len = 50 # chunk length of the data (the number of characters we want in the sequence of the training data)\n",
    "\n",
    "# Instantiate the model object, optimizer and loss function\n",
    "rnn = RNN(n_characters, hidden_size, n_characters, n_layers) # feed the number of characters, hidden size, number of characters, and number of layers into the RNN model\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr) # adam is the most popular loss function of the optimizer, and we are using it here\n",
    "loss_fn = nn.CrossEntropyLoss() # cross entropy loss function\n",
    "\n",
    "# Run the training\n",
    "for epoch in range(1, n_epochs + 1): # loop through the number of epochs\n",
    "    loss = train(*random_training_set()) # Calculate training loss\n",
    "    valid_loss = evaluate(*random_valid_set())  # Calculate test loss\n",
    "    if epoch % print_every == 0: # print the loss every 1000 epochs\n",
    "        print('Epoch: %d, Training Loss: %.4f, Validation Loss: %.4f' % (epoch, loss, valid_loss)) # print the epoch, training loss, and test loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be032b8-5803-4aac-9e65-5294c43f4dea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate text with this model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aaf1ae0-af28-4885-a8ea-e9f78b49bed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(decoder, prime_str='ch', predict_len=100, temperature=0.8): # generate text by feeiding the model 'decoder', the string we want to start with 'prime_str', the number of characters we want to predict 'predict_len', and the strictness vs creative freedom of the model 'temperature'\n",
    "    hidden = decoder.init_hidden() # initialize the previous hidden state to start the prediction\n",
    "    prime_input = char_tensor(prime_str) # convert the prime string 'ch' to a tensor\n",
    "    predicted = prime_str # the first predicted character has to be the prime string\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state if there are two characters in the prime string\n",
    "    for p in range(len(prime_str) - 1): # loop through the prime string\n",
    "        _, hidden = decoder(prime_input[p], hidden) # the input is whatever tensors we created except the last one, and the hidden state is the previous hidden state\n",
    "    inp = prime_input[-1] # start predicting the next character by taking the last character in the prime string\n",
    "    \n",
    "    for p in range(predict_len): # loop through the number of characters we want to predict\n",
    "        output, hidden = decoder(inp, hidden) # feed the input character and the hidden state into the RNN model to get the output and hidden state\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp() # look at the data of the output, divide by temperature to get the strictness vs creative freedom, and exponentiate it to get the multinomial output distribution probabilities\n",
    "        top_i = torch.multinomial(output_dist, 1)[0] # take the top index of the multinomial distribution (the character with the highest probability)\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i] # take the index of the character with the highest probability and get the corresponding character as the predicted character\n",
    "        predicted += predicted_char # add the predicted character to the predicted string\n",
    "        inp = char_tensor(predicted_char) # convert the predicted character to a tensor to be fed back into the model for the next iteration\n",
    "\n",
    "    return predicted # return the predicted string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff05865a-43d7-44ec-a548-ba84ce2e93c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charger dataset. It contains many more sentences and characes and chara larger dataset. It contains ma\n"
     ]
    }
   ],
   "source": [
    "print(generate(rnn)) # the rnn function has been trained on the dataset, and now we are generating text using the trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
