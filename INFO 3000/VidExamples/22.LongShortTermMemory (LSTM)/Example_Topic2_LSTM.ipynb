{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398c2c48-1dd1-483e-9eff-e796b775299d",
   "metadata": {},
   "source": [
    "### LSTM autocomplete example - a very simple and limited version ###\n",
    "\n",
    "This will train a simple LSTM to predict the next word given a sequence of words. Please note that this is a very simple example and in a real-world scenario, you would need a lot more data, data preprocessing (like handling of unknown words, punctuation, and so on), and often more complex models and training procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9523f8ae-e798-47c4-9b4d-9de9a3eac595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import torch # PyTorch\n",
    "import torch.nn as nn # PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee88b348-7ff5-4280-a335-75c33ee75cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Synthetic training data\n",
    "\n",
    "data = \"hello world. this is an example to demonstrate lstm. hope you enjoy the example.\".split() # tokenie the data by splitting the words\n",
    "\n",
    "\n",
    "words = set(data) # take a union where every word that is duplicated appears only once\n",
    "word2idx = {word: i for i, word in enumerate(words)} # (enumerate - gets both the word and index) - create a dictionary where the key is the word and the value is the index\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "# Prepare input and target sequences\n",
    "sequence_length = 2 # take the first two words as input and then learn to predict the third word\n",
    "x_data = [] # input sequence\n",
    "y_data = [] # target sequence\n",
    "\n",
    "for i in range(0, len(data) - sequence_length): # iterate from the first word to the second last word\n",
    "    sequence = data[i:i + sequence_length] # take the first two words as features\n",
    "    target = data[i + sequence_length] # take the third word as the target/label\n",
    "    x_data.append([word2idx[word] for word in sequence]) # append the index of the words in the sequence\n",
    "    y_data.append(word2idx[target]) # append the index of the target word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd045ea8-df17-4953-b583-0c375b4e52b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words list after tokenizing:\n",
      "{'an', 'is', 'demonstrate', 'world.', 'example', 'hope', 'example.', 'lstm.', 'this', 'the', 'to', 'enjoy', 'hello', 'you'}\n",
      "\n",
      "Generating the vocabulary and a numeric representation for each word:\n",
      "{'an': 0, 'is': 1, 'demonstrate': 2, 'world.': 3, 'example': 4, 'hope': 5, 'example.': 6, 'lstm.': 7, 'this': 8, 'the': 9, 'to': 10, 'enjoy': 11, 'hello': 12, 'you': 13}\n",
      "\n",
      "Reversing it to retrieve the word once the index is know:\n",
      "{0: 'an', 1: 'is', 2: 'demonstrate', 3: 'world.', 4: 'example', 5: 'hope', 6: 'example.', 7: 'lstm.', 8: 'this', 9: 'the', 10: 'to', 11: 'enjoy', 12: 'hello', 13: 'you'}\n",
      "\n",
      "Creating the  sequence of features and labels from our training data as per sequence length. Features:\n",
      "[[12, 3], [3, 8], [8, 1], [1, 0], [0, 4], [4, 10], [10, 2], [2, 7], [7, 5], [5, 13], [13, 11], [11, 9]]\n",
      "Label:\n",
      "[8, 1, 0, 4, 10, 2, 7, 5, 13, 11, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "# Let us understand what the above cell has done\n",
    "\n",
    "print(f\"Words list after tokenizing:\\n{words}\\n\") # list of unique words\n",
    "\n",
    "print(f\"Generating the vocabulary and a numeric representation for each word:\\n{word2idx}\\n\") # dictionary of words and their corresponding index\n",
    "print(f\"Reversing it to retrieve the word once the index is know:\\n{idx2word}\") # dictionary of index and the corresponding word\n",
    "\n",
    "print(f\"\\nCreating the  sequence of features and labels from our training data as per sequence length. Features:\\n{x_data}\\nLabel:\\n{y_data}\") # sequence of features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899881d9-83ed-4d1a-9c4f-dbf3406f0320",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The LSTM in PyTorch, specified as `torch.nn.LSTM`, expects the following inputs: ####\n",
    "\n",
    "1. **input**: The input tensor of shape `(seq_len, batch, input_size)`, where `seq_len` is the length of the sequence, `batch` is the batch size, and `input_size` is the number of features in the input. If the `batch_first` argument is `True`, the input is expected to be of shape `(batch, seq_len, input_size)`.\n",
    "\n",
    "2. **h_0**: The initial hidden state for each element in the batch, of shape `(num_layers * num_directions, batch, hidden_size)`. This is an optional argument. If not provided, it defaults to a tensor of zeros.\n",
    "\n",
    "3. **c_0**: The initial cell state for each element in the batch, of shape `(num_layers * num_directions, batch, hidden_size)`. This is also optional. If not provided, it defaults to a tensor of zeros.\n",
    "\n",
    "The LSTM returns the following outputs:\n",
    "\n",
    "1. **output**: The output features from the last layer of the LSTM for each time step, returned as a tensor of shape `(seq_len, batch, num_directions * hidden_size)`. If `batch_first` is `True`, the output will be of shape `(batch, seq_len, num_directions * hidden_size)`.\n",
    "\n",
    "2. **h_n**: The hidden state for the last time step, returned as a tensor of shape `(num_layers * num_directions, batch, hidden_size)`.\n",
    "\n",
    "3. **c_n**: The cell state for the last time step, returned as a tensor of shape `(num_layers * num_directions, batch, hidden_size)`.\n",
    "\n",
    "Here, `num_layers` refers to the number of stacked LSTM layers you have (as specified in the LSTM constructor), and `num_directions` is 2 for bidirectional LSTMs and 1 for unidirectional LSTMs.\n",
    "\n",
    "Note: In many cases, especially when working with sequence data of variable lengths, you will also need to make use of `torch.nn.utils.rnn.pack_padded_sequence` and `torch.nn.utils.rnn.pad_packed_sequence` to pack and unpack sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951aa324-0b2c-4885-83ce-903b2e17d4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "\n",
    "class LSTM(nn.Module): # create a class LSTM that inherits from nn.Module\n",
    "    def __init__(self, input_size, hidden_size, output_size): # take in input_size - the number of features (words) in the input, hidden_size - the initial state for each element in the batch (how long the vector will be), and output_size as parameters for the constructor\n",
    "        super(LSTM, self).__init__() # call the constructor of the parent class\n",
    "        self.hidden_size = hidden_size # set the hidden_size to the hidden_size passed as a parameter\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) # the inputs are vectorized\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True) # instantiate the LSTM layer, by sending in the hidden_size as the input size, hidden_size as the number of neurons, and batch_first as True so that the input and output tensors are provided as (batch, seq, feature)\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # create a fully connected layer that takes in the hidden_size as input and output_size as output\n",
    "\n",
    "    def forward(self, x): # take in the input tensor x for the forward pass\n",
    "        x = self.embedding(x) # embed the input tensor\n",
    "        out, _ = self.lstm(x) # run the LSTM layer on the embedded input tensor\n",
    "        out = self.fc(out[:, -1, :]) # take the last output from the LSTM layer and pass it through the fully connected layer\n",
    "        return out # return the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbacb3b0-693f-4543-a069-8ab5e31fa6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters initializing\n",
    "\n",
    "input_size = len(words) # the number of unique words\n",
    "hidden_size = 50 # the number of neurons in the hidden layer\n",
    "output_size = len(words) # the number of unique words\n",
    "learning_rate = 0.001 # the learning rate\n",
    "\n",
    "# Convert data to tensors\n",
    "x_data = torch.tensor(x_data, dtype=torch.long) # convert x data into a torch tensor of type long\n",
    "y_data = torch.tensor(y_data, dtype=torch.long) # convert y data into a torch tensor of type long\n",
    "\n",
    "# Create the model\n",
    "model = LSTM(input_size, hidden_size, output_size) # instantiate the LSTM model\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # instantiate the CrossEntropyLoss function as the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # instantiate the Adam optimizer with the model parameters and learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7169675-9f51-4409-9744-0d7338ae7be4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 0.1769\n",
      "Epoch [200/2000], Loss: 0.0274\n",
      "Epoch [300/2000], Loss: 0.0119\n",
      "Epoch [400/2000], Loss: 0.0069\n",
      "Epoch [500/2000], Loss: 0.0045\n",
      "Epoch [600/2000], Loss: 0.0033\n",
      "Epoch [700/2000], Loss: 0.0025\n",
      "Epoch [800/2000], Loss: 0.0019\n",
      "Epoch [900/2000], Loss: 0.0016\n",
      "Epoch [1000/2000], Loss: 0.0013\n",
      "Epoch [1100/2000], Loss: 0.0011\n",
      "Epoch [1200/2000], Loss: 0.0009\n",
      "Epoch [1300/2000], Loss: 0.0008\n",
      "Epoch [1400/2000], Loss: 0.0007\n",
      "Epoch [1500/2000], Loss: 0.0006\n",
      "Epoch [1600/2000], Loss: 0.0005\n",
      "Epoch [1700/2000], Loss: 0.0005\n",
      "Epoch [1800/2000], Loss: 0.0004\n",
      "Epoch [1900/2000], Loss: 0.0004\n",
      "Epoch [2000/2000], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# We never initialized the hidden layer and the cell state, telling us that if we do not do it ourselves, the model will initialize it with zeros as soon as we instantiate the model\n",
    "num_epochs = 2000 # the number of epochs\n",
    "for epoch in range(num_epochs): # iterate over the number of epochs\n",
    "    outputs = model(x_data) # feed the input x data into the model to get the outputs\n",
    "    loss = criterion(outputs, y_data) # calculate the loss by comparing the outputs and label data\n",
    "\n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() # backpropagate the loss\n",
    "    optimizer.step() # update the parameters\n",
    "\n",
    "    if (epoch+1) % 100 == 0: # print the loss every 100 epochs\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item())) # print the epoch number, number of epochs, and the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2753b69-901f-4140-8760-9d6a2054c543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function will take a sequence of words and \n",
    "# return the word that the model thinks is the most likely to come next.\n",
    "\n",
    "def predict(model, sequence): # take the model and the sequence as parameters\n",
    "    sequence = [word2idx[word] for word in sequence] # convert the sequence into a list of indices\n",
    "    sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0) # convert the sequence into a tensor of type long and add a dimension at the beginning\n",
    "    output = model(sequence) # feed the sequence tensor into the model to get the output\n",
    "    _, predicted = torch.max(output.data, 1) # get the index of the word with the highest probability\n",
    "    return idx2word[predicted.item()] # return the word corresponding to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe22b53-2118-42af-81f4-10d355f684c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n"
     ]
    }
   ],
   "source": [
    "# Feed in two words of the same sequence length as the model was trained on\n",
    "\n",
    "sequence = [\"this\", \"is\"] # take the first two words of the sequence, where the words should be in the vocabulary of the model, else it will throw an error\n",
    "print(predict(model, sequence))  # Output next word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6ddf6-f413-47a4-9f40-2e6d98ed1ae1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The predict function expects the sequence to be of the same length as was used for training (sequence_length), and all the words in the sequence must be in the training data, otherwise, they will not be in word2idx dictionary and you'll get a KeyError. \n",
    "\n",
    "In a real-world scenario, you'd want to add more sophisticated handling of such cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
