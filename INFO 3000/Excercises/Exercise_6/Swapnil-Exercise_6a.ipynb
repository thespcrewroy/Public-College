{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6a Template - RNN implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Import libraries'''\n",
    "import numpy as np # import numpy library as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous state input weights:\n",
      " [[ 1.03099952  0.93128012 -0.83921752 -0.30921238]\n",
      " [ 0.33126343  0.97554513 -0.47917424 -0.18565898]\n",
      " [-1.10633497 -1.19620662  0.81252582  1.35624003]\n",
      " [-0.07201012  1.0035329   0.36163603 -0.64511975]]\n",
      "\n",
      " Weights for thw input x:\n",
      "[[-1.72491783 -0.56228753 -1.01283112  0.31424733 -0.90802408 -1.4123037\n",
      "   1.46564877 -0.2257763   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      "  -0.60170661  1.85227818 -0.01349722 -1.05771093]\n",
      " [ 0.82254491 -1.22084365  0.2088636  -1.95967012 -1.32818605  0.19686124\n",
      "   0.73846658  0.17136828 -0.11564828 -0.3011037 ]\n",
      " [-1.47852199 -0.71984421 -0.46063877  1.05712223  0.34361829 -1.76304016\n",
      "   0.32408397 -0.38508228 -0.676922    0.61167629]]\n",
      "\n",
      "Weights for the prediction y:\n",
      "[[ 0.36139561  1.53803657 -0.03582604  1.56464366]\n",
      " [-2.6197451   0.8219025   0.08704707 -0.29900735]\n",
      " [ 0.09176078 -1.98756891 -0.21967189  0.35711257]\n",
      " [ 1.47789404 -0.51827022 -0.8084936  -0.50175704]\n",
      " [ 0.91540212  0.32875111 -0.5297602   0.51326743]]\n",
      "\n",
      "Bias:\n",
      "[ 0.09707755  0.96864499 -0.70205309 -0.32766215]\n",
      "Bias for predicted output[[ 0.36139561  1.53803657 -0.03582604  1.56464366]\n",
      " [-2.6197451   0.8219025   0.08704707 -0.29900735]\n",
      " [ 0.09176078 -1.98756891 -0.21967189  0.35711257]\n",
      " [ 1.47789404 -0.51827022 -0.8084936  -0.50175704]\n",
      " [ 0.91540212  0.32875111 -0.5297602   0.51326743]]\n",
      "\n",
      "\n",
      "Shape of Wa: (4, 4), of Wx(4, 10), of Wy: (5, 4), bias: (4,), predicted output bias: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Initialising the Variables\n",
    "\n",
    "n_features = 10 # Input which has 10 features (like the numeric representation of a word - word embedding)\n",
    "n_hidden_units = 4 # 4 neurons\n",
    "\n",
    "# Number of predicted outputs (y_t)\n",
    "n_output_units = 5  # for example, 5 classes in a classification problem\n",
    "\n",
    "# Random initialization for the sake of example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input, Weights, Bias\n",
    "x_t = np.random.randn(n_features) # Random input vector\n",
    "h_prev = np.random.randn(n_hidden_units) # Random hidden state vector\n",
    "Wx = np.random.randn(n_hidden_units, n_features) # Random weights for the input x\n",
    "Wh = np.random.randn(n_hidden_units, n_hidden_units) # Random weights for the previous state\n",
    "Wy = np.random.randn(n_output_units, n_hidden_units) # Random weights for the prediction y\n",
    "b = np.random.randn(n_hidden_units) # Random bias\n",
    "by = np.random.randn(n_output_units) # Random bias for the predicted output\n",
    "\n",
    "print(f\"Previous state input weights:\\n {Wh}\\n\\n Weights for thw input x:\\n{Wx}\\n\\nWeights for the prediction y:\\n{Wy}\\n\\nBias:\\n{b}\\nBias for predicted output{Wy}\")\n",
    "print(f\"\\n\\nShape of Wa: {Wh.shape}, of Wx{Wx.shape}, of Wy: {Wy.shape}, bias: {b.shape}, predicted output bias: {by.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function is given - understand what it does\n",
    "# It ultimately converts outputs to probability values\n",
    "\n",
    "def softmax(z): # z is the input vector\n",
    "    t = np.exp(z) # take the exponential of the input vector\n",
    "    a = np.exp(z) / np.sum(t, axis=1).reshape(-1,1) # divide the exponential of the input vector by the reshaped sum of the exponential of the input vector\n",
    "    return a # return the softmax of the input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to take the inputs as previous state,x and bias values to return the value of the next state using the RNN equation\n",
    "# Always check the dimensions of your results\n",
    "\n",
    "# RNN Basic function\n",
    "\n",
    "# TODO \n",
    "\n",
    "'''A RNN cell gets 2 inputs - One from the previous timestep output (a^(t-1)) and One the current timestep input (x^t) with their wieghts'''\n",
    "def rnn_step(x_t, h_prev, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Perform a single RNN step.\n",
    "\n",
    "    Arguments:\n",
    "    x_t -- Current input data for timestep t, numpy array of shape (n_features,)\n",
    "    h_prev -- Previous hidden state, numpy array of shape (n_hidden_units,)\n",
    "    Wx -- Weight matrix multiplying the input, numpy array of shape (n_hidden_units, n_features)\n",
    "    Wh -- Weight matrix multiplying the hidden state, numpy array of shape (n_hidden_units, n_hidden_units)\n",
    "    b -- Bias vector, numpy array of shape (n_hidden_units,)\n",
    "\n",
    "    Returns:\n",
    "    h_next -- Next hidden state, numpy array of shape (n_hidden_units,)\n",
    "    \"\"\"\n",
    "    # Compute the next hidden state\n",
    "    \n",
    "    #TODO\n",
    "\n",
    "    a_prev = h_prev # previous timestep input (hidden state)\n",
    "    W_aa = Wh # weight of previous timestep input (hidden state)\n",
    "    x_t = x_t # current timestep input\n",
    "    W_ax = Wx # weight of current timestep input\n",
    "    b_a = b # bias\n",
    "\n",
    "        \n",
    "    '''Their respective  dot products are taken, and the two inputs are added together with the bias (as per equation for a<t>.)'''\n",
    "    a_t = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n",
    "\n",
    "    return a_t\n",
    "        \n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next hidden state (a_next): [ 0.46019631  0.2484162  -0.99331655  0.98477645]\n",
      "Shape of a_next: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Compute next state using the RNN equation developed in the function\n",
    "# print the values and the shape\n",
    "# make sure the shape (dimension of output is what it should be. In this case (4,1)\n",
    "\n",
    "# TO DO\n",
    "'''They are then fed to a tanh activation function to get the output a<t> which goes as input to the next time step.'''\n",
    "x_t = x_t # reshape the input\n",
    "a_prev = h_prev # reshape the previous timestep input (hidden state)\n",
    "W_ax = Wx # weight of current timestep input\n",
    "W_aa = Wh # weight of previous timestep input (hidden state)\n",
    "b_a = b # bias\n",
    "\n",
    "a_t = rnn_step(x_t, a_prev, W_ax, W_aa, b) # get the next hidden state\n",
    "\n",
    "print(f\"Next hidden state (a_next): {a_t}\")\n",
    "print(\"Shape of a_next:\", a_t.shape)\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted output y:\n",
      "[ 1.73268879 -2.84585877  0.41448371  1.1214015   1.53971849]\n",
      "Predicted prbabilites of classes for this RNN Cell:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Shape: (5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Compute the prediction for the particular cell, which is y(t)\n",
    "# Use the provided softmax function\n",
    "# Feed the softmax the value of y_t but make sure it has a shape (5,1) and not just (5,)\n",
    "# Print the output as the probabilites of number of classification classes\n",
    "# Since the classes are 5 the output dimension should be the same.\n",
    "\n",
    "y_t = np.dot(Wy, a_t) + by\n",
    "\n",
    "y_probailities_of_classes = softmax(y_t.reshape(-1,1)) # Reshaping is done to make sure the array being fed is (n,1)\n",
    "\n",
    "print(f\"The predicted output y:\\n{y_t}\")\n",
    "print(f\"Predicted prbabilites of classes for this RNN Cell:\\n{y_probailities_of_classes}\\nShape: {y_probailities_of_classes.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
